apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: test-vm-cmd
spec:
  description: Run a command in a test VM
  params:
  - name: PLATFORM
    description: The platform of the VM to provision
  - name: image
    description: The image to use when setting up the test environment
  - name: cmd
    description: The command to run
  - name: envs
    description: List of environment variables (NAME=VALUE) to be set in the test environment
    type: array
    default: []
  results:
  - name: TEST_OUTPUT
    description: Test result in json format
  volumes:
  - name: workdir
    emptyDir: {}
  - name: ssh
    secret:
      secretName: multi-platform-ssh-$(context.taskRun.name)
  steps:
  - name: run-in-vm
    image: registry.access.redhat.com/ubi10/ubi:latest
    volumeMounts:
    - mountPath: /var/workdir
      name: workdir
    - mountPath: /ssh
      name: ssh
    workingDir: /var/workdir
    env:
    - name: TEST_IMAGE
      value: $(params.image)
    - name: TEST_CMD
      value: $(params.cmd)
    - name: RESULTS_TEST_OUTPUT_PATH
      value: $(results.TEST_OUTPUT.path)
    args:
    - $(params.envs[*])
    script: |
      #!/bin/bash -ex
      log() {
        echo "[$(date -uIns)]" $*
      }

      log Install packages
      dnf -y install openssh-clients rsync jq

      log Prepare connection

      if [ -e "/ssh/error" ]; then
        log Error provisioning VM
        cat /ssh/error
        exit 1
      fi
      export SSH_HOST=$(cat /ssh/host)

      mkdir -p ~/.ssh
      if [ "$SSH_HOST" == "localhost" ] ; then
        IS_LOCALHOST=true
        log Localhost detected, running build in cluster
      elif [ -s "/ssh/otp" ]; then
        log Fetching OTP token
        curl --cacert /ssh/otp-ca -d @/ssh/otp $(cat /ssh/otp-server) > ~/.ssh/id_rsa
        echo >> ~/.ssh/id_rsa
        chmod 0400 ~/.ssh/id_rsa
      elif [ -s "/ssh/id_rsa" ]; then
        log Copying ssh key
        cp /ssh/id_rsa ~/.ssh
        chmod 0400 ~/.ssh/id_rsa
      else
        log No authentication mechanism found
        exit 1
      fi

      mkdir -p scripts

      PODMAN_ENV=()
      while [ $# -ne 0 ]; do
        PODMAN_ENV+=("-e" "$1")
        shift
      done

      cat > scripts/test.sh <<SCRIPTEOF
      #!/bin/bash -ex
      EXTRA_ARGS=()
      if [ -f /etc/cdi/nvidia.yaml ]; then
        echo "Found NVIDIA config, enabling CDI"
        EXTRA_ARGS+=("--device" "nvidia.com/gpu=all")
        EXTRA_ARGS+=("-v" "/dev/dri:/dev/dri")
        if [ -x /usr/bin/nvidia-smi ]; then
          nvidia-smi -L
        fi
        curl -fsSL https://github.com/mikefarah/yq/releases/download/v4.47.2/yq_linux_amd64.tar.gz | tar -xzv
        # Delete the CDI hook which mounts /proc/driver/nvidia/params into the container,
        # preventing mounting of a new /proc when running podman-in-podman.
        ./yq_linux_amd64 -i --indent=4 \
          'del(.containerEdits.hooks[] | select(.args[] == "disable-device-node-modification"))' \
          /etc/cdi/nvidia.yaml
        # Some files are mounted from /usr/share/vulkan into /etc/vulkan in the outer container.
        # Remap their source paths to /etc/vulkan so they can be found by podman-in-podman.
        sed -e 's|/usr/share/vulkan|/etc/vulkan|' /etc/cdi/nvidia.yaml > /tmp/nvidia.yaml
        # Delete all hooks from the podman-in-podman CDI config, they are unnecessary and fail because
        # the nested container is not running as root.
        ./yq_linux_amd64 -i --indent=4 \
          'del(.containerEdits.hooks[]) | del(.devices[].containerEdits.hooks[])' \
          /tmp/nvidia.yaml
        EXTRA_ARGS+=("-v" "/tmp/nvidia.yaml:/etc/cdi/nvidia.yaml")
      fi
      podman run \
      --userns=keep-id \
      --security-opt label=disable \
      --security-opt unmask=/proc/* \
      --device /dev/net/tun \
      -v /tmp \
      \${EXTRA_ARGS[*]} \
      ${PODMAN_ENV[*]} \
      $TEST_IMAGE $TEST_CMD
      SCRIPTEOF
      chmod +x scripts/test.sh

      if ! [[ $IS_LOCALHOST ]]; then
        log VM exec
        export BUILD_DIR=$(cat /ssh/user-dir)
        export SSH_ARGS="-o StrictHostKeyChecking=no -o ServerAliveInterval=60 -o ServerAliveCountMax=10"
        # ssh once before rsync to retrieve the host key
        ssh $SSH_ARGS "$SSH_HOST" "uname -a"
        rsync -ra scripts "$SSH_HOST:$BUILD_DIR"
        ssh $SSH_ARGS "$SSH_HOST" "$BUILD_DIR/scripts/test.sh"
        log End VM exec
      else
        log Local exec
        scripts/test.sh
        log End local exec
      fi

      jq -jnc '{result: "SUCCESS", timestamp: now | todateiso8601, failures: 0, successes: 1, warnings: 0}' | tee "$RESULTS_TEST_OUTPUT_PATH"
      echo
