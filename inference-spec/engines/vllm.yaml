schema_version: "1.0.1"
commands:
  - name: serve
    inference_engine:
      name: "vllm server"
      native_cli:
        binary: python3
        args: ["-m", "vllm.entrypoints.openai.api_server"]
      container_cli: {}
      options: &serve_run_options
        - name: "--model"
          description: "The AI model to run"
          value: "{{ model.model_path }}"
        - name: "--served-model-name"
          description: "The name assigned to the run AI model"
          value: "{{ model.alias }}"
        - name: "--max_model_len"
          description: "Size of the model context"
          value: "{{ args.ctx_size if args.ctx_size else 2048 }}"
        - name: "--port"
          description: "Port for the AI model server to listen on"
          value: "{{ args.port }}"
        - name: "--seed"
          description: "Seed the global PRNG"
          value: "{{ args.seed }}"
        # Special case:
        # Pass arbitrary runtime arguments to vLLM server
        - name: ""
          description: "Arbitrary runtime arguments for vLLM server"
          value: "{{ args.runtime_args }}"
  - name: run
    inference_engine:
      name: "vllm server with chat"
      native_cli:
        binary: python3
        args: ["-m", "vllm.entrypoints.openai.api_server"]
      container_cli: {}
      options: *serve_run_options
